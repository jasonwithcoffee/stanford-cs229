\item \points{20} {\bf Binary Classification: Experimenting with Activation Functions}\\~\\
\textbf{Note: }Although this problem is related to neural networks, we have specifically included it to allow you to practice the mathematical prerequisites for XCS229i and provide an opportunity for you to apply them in a real-world binary classification problem. Some minimal knowledge about the structure of a neural network is required to complete the problem. This information can be obtained in the Deep Learning handout \href{https://stanford.box.com/s/6qdftyrji89js0uvg4ec23jfmj21xemd}{(link)}. We will cover more complex neural networks in future lectures and problem sets.
\\~\\

Let $X = \{x^{(1)}, \cdots, x^{(\nexp)}\}$ be a dataset of $\nexp$ samples with 2 features, i.e $\xsi \in \R^2$. The samples are classified into 2 categories with labels $\ysi \in \{0, 1\}$. A scatter plot of the dataset is shown in Figure $\ref{fig:nn_plot}$:
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3]{simple_nn/nn_plot.png}
    \caption{Plot of dataset $X$.}
    \label{fig:nn_plot}
\end{figure}

The examples in class $1$ are marked as as ``$\times$" and examples in class $0$ are marked as ``$\circ$". We want to perform binary classification using a simple neural network with the architecture shown in Figure $\ref{fig:nn_arc}$:
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.3, clip]{simple_nn/nn_architecture.png}
    \caption{Architecture for our simple neural network.}
     \label{fig:nn_arc}
\end{figure}

Denote the two features $x_1$ and $x_2$, the three neurons in the hidden layer $h_1, h_2$, and $h_3$, and the output neuron as $o$. Let the weight from $x_i$ to $h_j$ be $w_{i, j}^{[1]}$ for $i \in \{1, 2\}, j \in \{1, 2, 3\}$, and the weight from $h_j$ to $o$ be $w_{j}^{[2]}$. Finally, denote the intercept weight for $h_j$ as $w_{0, j}^{[1]}$, and the intercept weight for $o$ as $w_{0}^{[2]}$. For the loss function, we'll use average squared loss instead of the usual negative log-likelihood:
$$l = \frac{1}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right)^2,$$
where $o^{(i)}$ is the result of the output neuron for example $i$.

\begin{enumerate}
  \input{simple_nn/01-sigmoid}
\newpage
\ifnum\solutions=1 {
  \input{simple_nn/01-sigmoid_sol}
} \fi

  \input{simple_nn/02-step_function}

\ifnum\solutions=1 {
  \input{simple_nn/02-step_function_sol}
} \fi

  \input{simple_nn/03-linear}
\ifnum\solutions=1 {
  \input{simple_nn/03-linear_sol}
} \fi


\end{enumerate}
