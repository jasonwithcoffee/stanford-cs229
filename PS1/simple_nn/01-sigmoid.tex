\item \subquestionpointswritten{5}
Suppose we use the sigmoid function as the activation function for $h_1, h_2, h_3$ and $o$.
What is the gradient descent update to $w_{1, 2}^{[1]}$, assuming we use a learning rate of $\alpha$?
Your answer should be written in terms of $\xsi$, $o^{(i)}$, $\ysi$, and the weights.\\

{\bf BEGIN PROOF HERE}\\

Let $g$ denote the sigmoid function $g(x) = \frac{1}{1 + e^{-x}}$, and $h^{(i)}_j$ denote the output of hidden neuron $h_j$ for sample $i$. For shorthand, we denote $\vec{w_j}^T\xsi = w_{1, j}^{[1]}x_1^{(i)} + w_{2, j}^{[1]}x_2^{(i)} + w_{0, j}^{[1]}$ for $j \in \{1, 2, 3\}$, and $\vec{w_o}^Th^{(i)} = w_{1}^{[2]}h^{(i)}_1 + w_{2}^{[2]}h^{(i)}_2 + w_{3}^{[2]}h^{(i)}_3 + w_{0}^{[2]}$. Using chain rule, we have \\
\begin{flalign*}
    \frac{\partial l}{\partial w_{1, 2}^{[1]}} 
    &= \frac{\partial}{\partial w_{1, 2}^{[1]}} \frac{1}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right)^2 \\
    &= \frac{1}{\nexp}\sum_{i=1}^{\nexp} \frac{\partial}{\partial w_{1, 2}^{[1]}} \left(o^{(i)} - \ysi\right)^2 \\
    &= \frac{2}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) \frac{\partial o^{(i)}}{\partial w_{1, 2}^{[1]}}
     & & & & & &\\[50pt]
    \frac{\partial (o^{(i)})}{\partial w_{1, 2}^{[1]}} 
    &= \frac{\partial (o^{(i)} (\vec{w_o}^Th^{(i)})) } {\partial w_{1, 2}^{[1]}} \\
    &= o^{(i)}(1-o^{(i)}) \frac{\partial (\vec{w_o}^Th^{(i)})) } {\partial w_{1, 2}^{[1]}} \\
    &= o^{(i)}(1-o^{(i)}) \frac{\partial (w_{1}^{[2]}h^{(i)}_1 + w_{2}^{[2]}h^{(i)}_2 + w_{3}^{[2]}h^{(i)}_3 + w_{0}^{[2]})} {\partial w_{1, 2}^{[1]}} \\
    &= o^{(i)}(1-o^{(i)}) \frac{\partial (w_{2}^{[2]}h^{(i)}_2 )} {\partial w_{1, 2}^{[1]}} \\
    &= o^{(i)}(1-o^{(i)})w_{2}^{[2]} \frac{\partial (h^{(i)}_2 )} {\partial w_{1, 2}^{[1]}} 
    & & & &\\[50pt]
    \frac{\partial (h^{(i)}_2)}{\partial w_{1, 2}^{[1]}} 
    &= \frac{\partial (h^{(i)}_2(\vec{w_j}^T\xsi))}{\partial w_{1, 2}^{[1]}} \\
    &= h^{(i)}_2(1-h^{(i)}_2)\frac{\partial (\vec{w_2}^T\xsi)}{\partial w_{1, 2}^{[1]}} \\
    &= h^{(i)}_2(1-h^{(i)}_2)\frac{\partial (w_{1, 2}^{[1]}x_1^{(i)} + w_{2, 2}^{[1]}x_2^{(i)} + w_{0, 2}^{[1]})}{\partial w_{1, 2}^{[1]}} \\
    &= h^{(i)}_2(1-h^{(i)}_2)\frac{\partial (w_{1, 2}^{[1]}x_1^{(i)})}{\partial w_{1, 2}^{[1]}} \\
    &= h^{(i)}_2(1-h^{(i)}_2)x_1^{(i)} 
    & & & &\\[50pt]
\end{flalign*}
Combining everything, we have
\begin{flalign*}
    \frac{\partial l}{\partial w_{1, 2}^{[1]}} 
    &= \frac{2}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) \frac{\partial o^{(i)}}{\partial w_{1, 2}^{[1]}} \\
    &= \frac{2}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) o^{(i)}(1-o^{(i)})w_{2}^{[2]} \frac{\partial (h^{(i)}_2 )} {\partial w_{1, 2}^{[1]}} \\
    &=  \frac{2}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) o^{(i)}(1-o^{(i)})w_{2}^{[2]} h^{(i)}_2(1-h^{(i)}_2)x_1^{(i)} \\
    &= \frac{2w_{2}^{[2]}}{\nexp}\sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) o^{(i)}(1-o^{(i)}) h^{(i)}_2(1-h^{(i)}_2)x_1^{(i)} \\
    & & & & &\\[50pt]
\end{flalign*}
and the update rule is \\[50pt]
\begin{flalign*}
w_{1, 2}^{[1]} := w_{1, 2}^{[1]} + \alpha \frac{2w_{2}^{[2]}}{\nexp} \sum_{i=1}^{\nexp} \left(o^{(i)} - \ysi\right) o^{(i)}(1-o^{(i)}) h^{(i)}_2(1-h^{(i)}_2)x_1^{(i)} 
\end{flalign*}
{\bf END PROOF}\\