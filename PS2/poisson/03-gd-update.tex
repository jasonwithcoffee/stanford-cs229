\item \subquestionpointswritten{7} For a training set $\{(x^{(i)}, y^{(i)});\, i=1,\ldots,\nexp\}$, let the log-likelihood of an example be $\log p(y^{(i)} | x^{(i)}; \theta)$. By taking the derivative of the log-likelihood with respect to $\theta_j$, derive the stochastic gradient ascent update rule for learning using a GLM model with Poisson responses $y$ and the canonical response function.\\

{\bf BEGIN PROOF HERE}\\

The log-likelihood of an example $(x^{(i)}, y^{(i)})$ is defined as $\ell(\theta) = \log p(y^{(i)} | x^{(i)}; \theta)$. To derive the stochastic gradient ascent rule, use the results in part (a) and the standard GLM assumption that $\eta = \theta^Tx$.
\begin{flalign*}
	\frac{\partial \ell(\theta)}{\partial \theta_j}
	&= \frac{\partial \log p(y^{(i)} | x^{(i)}; \theta)}{\partial \theta_j}\\
	&= \frac {\partial \log \left({\frac{1}{y^{(i)}!} \exp(\eta^T y^{(i)} -
	e^\eta)}\right)} {\partial \theta_j}\\\\
    &= \frac {\partial}{\partial \theta_j} \left(
    	\log(\frac{1}{y!}) +\eta^T y^{(i)} -e^\eta
    	\right) \\
    &= \frac {\partial}{\partial \theta_j} \left(
    	\log(\frac{1}{y!}) +\theta^Tx^{(i)} y^{(i)} -e^{\theta^Tx^{(i)}}
    	\right) \\
    &= y^{(i)}x^{(i)} - e^{\theta^Tx^{(i)}}x^{(i)} \\
    &= \left(y^{(i)} - e^{\theta^Tx^{(i)}} \right)x^{(i)}
    & & & & &\\[50pt]
\end{flalign*}

The stochastic gradient ascent update rule is
%
\begin{equation*}
\theta_j := \theta_j + \alpha \frac{\partial \ell(\theta)}{\partial \theta_j},
\end{equation*}
%
which reduces here to:\\[50pt]
$ \theta_j := \theta_j + \alpha\left(y^{(i)} - e^{\theta^Tx^{(i)}} \right)x^{(i)} $ \\[20pt]
{\bf END PROOF}\\